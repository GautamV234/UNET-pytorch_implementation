{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNET.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "occl6csn9ckR"
      },
      "source": [
        "#####################\n",
        "#  RUN ONLY ONCE\n",
        "#####################\n",
        "\n",
        "\n",
        "# !pip install --upgrade albumentations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeG30p79arfN"
      },
      "source": [
        "# model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels):\n",
        "    super(DoubleConv,self).__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=3,stride=1,padding=1,bias=False),\n",
        "        nn.BatchNorm2d(num_features=out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(in_channels=out_channels,out_channels=out_channels,kernel_size=3,stride=1,padding=1,bias=False),\n",
        "        nn.BatchNorm2d(num_features=out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.conv(x)\n",
        "\n",
        "\n",
        "class UNET(nn.Module):\n",
        "  def __init__(self,in_channels=3,out_channels = 1,features=[64,128,256,512]):\n",
        "    super(UNET,self).__init__()\n",
        "    self.downs = nn.ModuleList()\n",
        "    self.ups = nn.ModuleList()\n",
        "    self.pool=nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "    # self.in_channels = in_channels\n",
        "    # self.out_channels = out_channels\n",
        "\n",
        "    # Down U part of the UNET\n",
        "    for feature in features:\n",
        "      self.downs.append(DoubleConv(in_channels,feature))\n",
        "      in_channels = feature\n",
        "\n",
        "    # Up U part of the UNET\n",
        "    for feature in reversed(features):\n",
        "      self.ups.append(\n",
        "          nn.ConvTranspose2d(\n",
        "          in_channels=feature*2,out_channels=feature,kernel_size=2,stride=2\n",
        "          )),\n",
        "      self.ups.append(DoubleConv(feature*2,feature)),\n",
        "    self.bottleneck = DoubleConv(features[-1],features[-1]*2)\n",
        "    self.final_conv = nn.Conv2d(features[0],out_channels,kernel_size=1)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    skip_connections = []\n",
        "    for down in self.downs:\n",
        "      # print(\"prev shape before down\")\n",
        "      print(x.shape)\n",
        "      x = down(x)\n",
        "      skip_connections.append(x)\n",
        "      # print(\"shape after down\")\n",
        "      # print(x.shape)\n",
        "      # print(\"##\")\n",
        "\n",
        "      x = self.pool(x)\n",
        "\n",
        "    x = self.bottleneck(x)\n",
        "    # print(\"x shape after bottleneck\")\n",
        "    # print(x.shape)\n",
        "    skip_connections = skip_connections[::-1]\n",
        "    for idx in range(0,len(self.ups),2):\n",
        "      # print(\"shape before up\")\n",
        "      # print(x.shape)\n",
        "      x=self.ups[idx](x)\n",
        "      # print(\"shape after up\")\n",
        "      # print(x.shape)\n",
        "      skip_connection = skip_connections[idx//2]\n",
        "      if x.shape !=skip_connection.shape:\n",
        "        # print(x.shape)\n",
        "        # print(skip_connection.shape)\n",
        "        # print(\"input output not same shape\")\n",
        "        x=TF.resize(x,size=skip_connection.shape[2:])\n",
        "      # print(\"x and skip_connections shape\")\n",
        "      # print(x.shape)\n",
        "      # print(skip_connection.shape)\n",
        "      concat_skip = torch.cat((skip_connection,x),dim=1)\n",
        "      # print(\"concat shape\")\n",
        "      # print(concat_skip.shape)\n",
        "      x= self.ups[idx+1](concat_skip)\n",
        "      # print(\"self.ups[idx+1]\")\n",
        "      # print(x.shape)\n",
        "    x = self.final_conv(x)\n",
        "    # print(\"final x shape\")\n",
        "    # print(x.shape)\n",
        "    return x\n",
        "\n",
        "def test():\n",
        "  x = torch.randn((2,1,530,390))\n",
        "  model = UNET(in_channels=1,out_channels=1)\n",
        "  preds=model(x)\n",
        "  print(x.shape)\n",
        "  print(preds.shape)\n",
        "  assert(preds.shape==x.shape)\n",
        "\n",
        "# if __name__=='__main__':\n",
        "#   test()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N89kOSw4gD1c"
      },
      "source": [
        "# Dataset \n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "class Nucleus_Dataset(Dataset):\n",
        "  def __init__(self,image_dir,mask_dir,transform=None):\n",
        "    self.image_dir = image_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "    self.images = os.listdir(image_dir)\n",
        "    self.masks = os.listdir(mask_dir)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "  \n",
        "  def __getitem__(self,index):\n",
        "    image_path = os.path.join(self.image_dir,self.images[index])\n",
        "    mask_path = os.path.join(self.mask_dir,self.masks[index])\n",
        "    image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
        "    mask = np.array(Image.open(mask_path).convert(\"L\"),dtype=np.float32)\n",
        "    mask[mask==255.0]=1.0\n",
        "\n",
        "    if self.transform is not None:\n",
        "      augmentations= self.transform(image=image,mask=mask)\n",
        "      image = augmentations[\"image\"]\n",
        "      mask = augmentations[\"mask\"]\n",
        "\n",
        "    return image,mask\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFWrfk6c2gdM"
      },
      "source": [
        "# Utils\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def save_checkpoint(state,filename=\"checkpoint.pth.tar\"):\n",
        "  print(\"Saving checkpoint =>\")\n",
        "  torch.save(state,filename)\n",
        "\n",
        "def load_checkpoint(checkpoint,model):\n",
        "  print(\"Loading Checkpoint =>\")\n",
        "  model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "\n",
        "def get_loader(\n",
        "    train_img_dir,\n",
        "    train_mask_dir,\n",
        "    val_img_dir,\n",
        "    val_mask_dir,\n",
        "    batch_size,\n",
        "    train_transforms,\n",
        "    val_transforms,\n",
        "    num_workers=4,\n",
        "    pin_memory=True):\n",
        "  train_ds = Nucleus_Dataset(train_img_dir,train_mask_dir,train_transforms)\n",
        "  train_loader = DataLoader(\n",
        "      train_ds,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=pin_memory,\n",
        "      shuffle=True)\n",
        "  val_ds = Nucleus_Dataset(val_img_dir,val_mask_dir,val_transforms)\n",
        "  val_loader = DataLoader(\n",
        "      val_ds,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=pin_memory,\n",
        "      shuffle=False)\n",
        "  return train_loader,val_loader\n",
        "\n",
        "\n",
        "def check_accuracy(loader,model,device=\"cuda\"):\n",
        "  num_correct=0\n",
        "  num_pixels=0\n",
        "  dice_score=0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for x,y in loader:\n",
        "      x = x.to(device=device)\n",
        "      y = y.to(device=device).unsqueeze(1)\n",
        "      preds = torch.sigmoid(model(x))\n",
        "      preds = (preds>0.5).float()\n",
        "      num_correct += (preds==y).sum()\n",
        "      num_pixels +=torch.numel(preds)\n",
        "      dice_score +=(2*(preds*y).sum())/(\n",
        "          (preds+y).sum() + 1e-8\n",
        "      ))\n",
        "\n",
        "  print(f\"Got {num_correct}/{num_pixels} with accuracy {num_correct*100/num_pixels:.2f}\")\n",
        "  print(f\"Dice Score: {dice_score/len(loader)}\")\n",
        "  model.train()\n",
        "\n",
        "\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK_acm8cyu7N"
      },
      "source": [
        "# train\n",
        "# !pip install --upgrade albumentations\n",
        "\n",
        "\n",
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 100\n",
        "NUM_WORKERS = 2\n",
        "IMAGE_HEIGHT=530\n",
        "IMAGE_WIDTH = 390\n",
        "PIN_MEMORY=True\n",
        "LOAD_MODEL = False\n",
        "Train_image_dir = \"\"\n",
        "Train_mask_dir = \"\"\n",
        "Val_image_dir = \"\"\n",
        "Val_mask_dir = \"\"\n",
        "\n",
        "def train_fnc(loader,model,optimizer,loss_fnc,scaler):\n",
        "  loop = tqdm(loader)\n",
        "  for batch_idx , (data,targets) in enumerate(loop):\n",
        "    data = data.to(device=DEVICE)\n",
        "    targets = targets.float().unsqueeze(1).to(device=DEVICE) \n",
        "    #unsqueezing to add a dimension for channel (batch_size,1,h,w)\n",
        "\n",
        "    # forward\n",
        "    with torch.cuda.amp.autocast():\n",
        "      preds = model(data)\n",
        "      loss = loss_fnc(preds,targets)\n",
        "    \n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    # update tqdm loop\n",
        "    loop.set_postfix(loss=loss.item())\n",
        "\n",
        "\n",
        "def main():\n",
        "  train_transform = A.Compose([\n",
        "                               A.Resize(height=IMAGE_HEIGHT,width = IMAGE_WIDTH),\n",
        "                               A.Rotate(limit=35,p=1.0),\n",
        "                               A.HorizontalFlip(p=0.5),\n",
        "                               A.VerticalFlip(p=0.1),\n",
        "                               A.Normalize(mean = [0.0,0.0,0.0],std=[1.0,1.0,1.0],max_pixel_value=255),\n",
        "                               ToTensorV2()\n",
        "\n",
        "  ])\n",
        "  val_transform = A.Compose([\n",
        "                               A.Resize(height=IMAGE_HEIGHT,width = IMAGE_WIDTH),\n",
        "                               A.Normalize(mean = [0.0,0.0,0.0],std=[1.0,1.0,1.0],max_pixel_value=255),\n",
        "                               ToTensorV2()\n",
        "\n",
        "  ])\n",
        "\n",
        "  model = UNET(in_channels=3,out_channels=1).to(device=DEVICE)\n",
        "  loss_fnc = nn.BCEWithLogitsLoss()\n",
        "  optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
        "\n",
        "  train_loader,val_loader = get_loader(\n",
        "      Train_image_dir,\n",
        "      Train_mask_dir,\n",
        "      Val_image_dir,\n",
        "      Val_mask_dir,\n",
        "      BATCH_SIZE,\n",
        "      train_transform,\n",
        "      val_transform,\n",
        "      NUM_WORKERS,\n",
        "      PIN_MEMORY \n",
        "  )\n",
        "\n",
        "  scaler = torch.cuda.amp.GradScaler()\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    train_fnc(train_loader,model,optimizer,loss_fnc,scaler)\n",
        "\n",
        "\n",
        "    # save model\n",
        "    # check accuracy\n",
        "    # save examples to folder\n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "  main()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkE3BHQx4UHP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}